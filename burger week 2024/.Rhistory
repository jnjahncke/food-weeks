burger_week <- tibble()
for (i in 1:length(rest_links)) {
url <- rest_links[i]
webpage <- read_html(url)
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
# build tibble
temp_tib = tibble(label = labels, info = info) |>
mutate(label2 = label_dict[label]) |>
select(-label) |> rename(label = label2) |>
pivot_wider(names_from = label, values_from = info)
if (dim(burger_week)[1] == 0) {
burger_week <- burger_week |> bind_rows(temp_tib)
} else {
burger_week <- burger_week |> full_join(temp_tib)
}
rm(info, url, webpage, labels, temp_tib)
}
i = 2
url <- rest_links[i]
webpage <- read_html(url)
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
labels
info
description <- html_nodes(webpage, ".description")
description
View(description)
xml_child(description[[1]], 37)
xml_child(description[[1]], 32)
description <- html_nodes(webpage, ".description") |> html_children()
description
description <- html_nodes(webpage, ".description") |> html_elements()
description <- html_nodes(webpage, ".description") |> html_text2() |> str_squish()
description
description <- html_nodes(webpage, ".description") |> html_text2() |> str_squish() |>
str_split_1(labels)
description
description <- html_nodes(webpage, ".description") |> html_text2() |> str_squish() |>
str_split(labels)
description
description <- html_nodes(webpage, ".description") |> html_text2() |> str_squish() |>
str_split_i(labels)
description <- html_nodes(webpage, ".description") |> html_text2() |> str_squish() |>
str_split_i(labels, length(labels))
description
description <- html_nodes(webpage, ".description") |> html_text2() |> str_squish() |>
str_split_fixed(labels, length(labels))
description <- html_nodes(webpage, ".description") |> html_text2() |> str_squish() |>
str_split_1(labels))
description <- html_nodes(webpage, ".description") |> html_text2() |> str_squish() |>
str_split_1(labels)
description <- html_nodes(webpage, ".description") |> html_text2() |> str_squish()
description
description <- html_nodes(webpage, ".description") |> html_text2()
description
description <- html_nodes(webpage, ".description") |> html_text2() |> str_split("\\n")
description
description[description != ""]
description[2]
description[description != NULL]
description[!is.na(description)]
description[lapply(description, length>0)]
description[lapply(description, length)>0]
purrr::compact(description)
description(2)
description[2]
description[2][[1]]
purrr::compact(description[[1]])
purrr::compact(description[[1]]) |> purrr::compact()
purrr::compact(description[[1]])[[2]]
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n") |> .[[1]]
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n") |> pull([[1]])
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description
description[description != ""]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
info2 <- c()
for (i in description) {
info2 = c(info2, str_split(i, labels[i]))
}
info2
info2
description
description[1]
description[1] |> str_split_1(labels[1])
description[1] |> str_split_1(labels[1]) |> compact()
description[1] |> str_split_1(labels[1])[[2]]
description[1] |> str_split_1(labels[1])
info2 <- list()
i = 1
info_temp <- str_split_1(labels[i])
info_temp <- str_split_1(description[i], labels[i])
info_temp
info2 = info2 + info_temp[2]
info2 <- c()
info_temp <- str_split_1(description[i], labels[i])
info2 = c(info2,info_temp[2])
info2 = c(info2,info_temp[2] |> str_squish())
url <- rest_links[i]
webpage <- read_html(url)
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
# build tibble
if (length(labels) != length(info)) {
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
info2 <- c()
for (i in seq(1,length(description))) {
info_temp <- str_split_1(description[i], labels[i])
info2 = c(info2,info_temp[2] |> str_squish())
}
info <- info2
}
temp_tib = tibble(label = labels, info = info) |>
mutate(label2 = label_dict[label]) |>
select(-label) |> rename(label = label2) |>
pivot_wider(names_from = label, values_from = info)
i = 2
url <- rest_links[i]
webpage <- read_html(url)
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
# build tibble
if (length(labels) != length(info)) {
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
info2 <- c()
for (j in seq(1,length(description))) {
info_temp <- str_split_1(description[j], labels[j])
info2 = c(info2,info_temp[2] |> str_squish())
}
info <- info2
}
temp_tib = tibble(label = labels, info = info) |>
mutate(label2 = label_dict[label]) |>
select(-label) |> rename(label = label2) |>
pivot_wider(names_from = label, values_from = info)
temp_tib
library(tidyverse)
library(rvest)
library(tidygeocoder)
library(sf)
library(googlesheets4)
url <- 'https://everout.com/portland/events/mercury-burger-week-2024/e180638/'
webpage <- read_html(url)
# get restaurant and burger names
restaurants <- html_nodes(webpage, '.text-center h4') |> html_text2()
burgers <- html_nodes(webpage, 'h3 a') |> html_text2()
# list of links to burger descriptions
rest_links <- html_nodes(webpage, 'h3 a') |> html_attr('href')
# label dictionary - will be used to name columns
label_dict <- c("What It's Called:" = "burger",
"What's On It:" = "toppings",
"What They Say About It:" = "description",
"Where and When To Get It:" = "address_hours",
"Jim Beam Drink Special:" = "JB_special",
# "Jim Beam Shot and Laurelwood Beer Drink Special:" = "JBL_special",
"The Fine Print!" = "fine_print",
"Allow Minors?" = "minors",
"Allow Takeout?" = "takeout",
"Allow Delivery?" = "delivery",
"Purchase Limit?" = "purchase_limit",
"Meat or Vegetarian?" = "meat_veggie",
"Available Gluten-Free?" = "gluten_free",
"Available Vegetarian?" = "veggie",
"Available Vegan?" = "vegan")
# loop through burger links, get info, build burger data table
burger_week <- tibble()
for (i in 1:length(rest_links)) {
url <- rest_links[i]
webpage <- read_html(url)
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
# build tibble
if (length(labels) != length(info)) {
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
info2 <- c()
for (j in seq(1,length(description))) {
info_temp <- str_split_1(description[j], labels[j])
info2 = c(info2,info_temp[2] |> str_squish())
}
info <- info2
}
temp_tib = tibble(label = labels, info = info) |>
mutate(label2 = label_dict[label]) |>
select(-label) |> rename(label = label2) |>
pivot_wider(names_from = label, values_from = info)
if (dim(burger_week)[1] == 0) {
burger_week <- burger_week |> bind_rows(temp_tib)
} else {
burger_week <- burger_week |> full_join(temp_tib)
}
rm(info, url, webpage, labels, temp_tib)
}
url <- rest_links[i]
webpage <- read_html(url)
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
if (length(labels) == 0) {
labels <- html_nodes(webpage, '.mb-5 strong') |> html_text2() |> str_squish()
}
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
# build tibble
if (length(labels) != length(info)) {
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
info2 <- c()
for (j in seq(1,length(description))) {
info_temp <- str_split_1(description[j], labels[j])
info2 = c(info2,info_temp[2] |> str_squish())
}
info <- info2
}
temp_tib = tibble(label = labels, info = info) |>
mutate(label2 = label_dict[label]) |>
select(-label) |> rename(label = label2) |>
pivot_wider(names_from = label, values_from = info)
if (dim(burger_week)[1] == 0) {
burger_week <- burger_week |> bind_rows(temp_tib)
} else {
burger_week <- burger_week |> full_join(temp_tib)
}
url <- rest_links[i]
webpage <- read_html(url)
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
if (length(labels) == 0) {
labels <- html_nodes(webpage, '.mb-5 strong') |> html_text2() |> str_squish()
}
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info
info <- info[info != "The Fine Print!"]
info
labels
# build tibble
if (length(labels) != length(info)) {
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
info2 <- c()
for (j in seq(1,length(description))) {
info_temp <- str_split_1(description[j], labels[j])
info2 = c(info2,info_temp[2] |> str_squish())
}
info <- info2
}
info2
description
labels
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
description
labels
url <- rest_links[i]
webpage <- read_html(url)
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
if (length(labels) == 0) {
labels <- html_nodes(webpage, '.mb-5 strong') |> html_text2() |> str_squish()
labels <- labels[labels != "The Fine Print!"]
}
# build tibble
if (length(labels) != length(info)) {
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
info2 <- c()
for (j in seq(1,length(description))) {
info_temp <- str_split_1(description[j], labels[j])
info2 = c(info2,info_temp[2] |> str_squish())
}
info <- info2
}
temp_tib = tibble(label = labels, info = info) |>
mutate(label2 = label_dict[label]) |>
select(-label) |> rename(label = label2) |>
pivot_wider(names_from = label, values_from = info)
temp_tib
# loop through burger links, get info, build burger data table
burger_week <- tibble()
for (i in 1:length(rest_links)) {
url <- rest_links[i]
webpage <- read_html(url)
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
if (length(labels) == 0) {
labels <- html_nodes(webpage, '.mb-5 strong') |> html_text2() |> str_squish()
labels <- labels[labels != "The Fine Print!"]
}
# build tibble
if (length(labels) != length(info)) {
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
info2 <- c()
for (j in seq(1,length(description))) {
info_temp <- str_split_1(description[j], labels[j])
info2 = c(info2,info_temp[2] |> str_squish())
}
info <- info2
}
temp_tib = tibble(label = labels, info = info) |>
mutate(label2 = label_dict[label]) |>
select(-label) |> rename(label = label2) |>
pivot_wider(names_from = label, values_from = info)
if (dim(burger_week)[1] == 0) {
burger_week <- burger_week |> bind_rows(temp_tib)
} else {
burger_week <- burger_week |> full_join(temp_tib)
}
rm(info, url, webpage, labels, temp_tib, info2, info_temp)
}
url <- rest_links[i]
webpage <- read_html(url)
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
if (length(labels) == 0) {
labels <- html_nodes(webpage, '.mb-5 strong') |> html_text2() |> str_squish()
labels <- labels[labels != "The Fine Print!"]
}
# build tibble
if (length(labels) != length(info)) {
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
info2 <- c()
for (j in seq(1,length(description))) {
info_temp <- str_split_1(description[j], labels[j])
info2 = c(info2,info_temp[2] |> str_squish())
}
info <- info2
}
info
labels
labels <- labels[labels != "The Fine Print!"]
labels <- labels[labels != "AND"]
# build tibble
if (length(labels) != length(info)) {
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
info2 <- c()
for (j in seq(1,length(description))) {
info_temp <- str_split_1(description[j], labels[j])
info2 = c(info2,info_temp[2] |> str_squish())
}
info <- info2
}
temp_tib = tibble(label = labels, info = info) |>
mutate(label2 = label_dict[label]) |>
select(-label) |> rename(label = label2) |>
pivot_wider(names_from = label, values_from = info)
# loop through burger links, get info, build burger data table
burger_week <- tibble()
for (i in 1:length(rest_links)) {
url <- rest_links[i]
webpage <- read_html(url)
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
if (length(labels) == 0) {
labels <- html_nodes(webpage, '.mb-5 strong') |> html_text2() |> str_squish()
}
labels <- labels[labels != "The Fine Print!"]
labels <- labels[labels != "AND"]
# build tibble
if (length(labels) != length(info)) {
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
info2 <- c()
for (j in seq(1,length(description))) {
info_temp <- str_split_1(description[j], labels[j])
info2 = c(info2,info_temp[2] |> str_squish())
}
info <- info2
}
temp_tib = tibble(label = labels, info = info) |>
mutate(label2 = label_dict[label]) |>
select(-label) |> rename(label = label2) |>
pivot_wider(names_from = label, values_from = info)
if (dim(burger_week)[1] == 0) {
burger_week <- burger_week |> bind_rows(temp_tib)
} else {
burger_week <- burger_week |> full_join(temp_tib)
}
rm(info, url, webpage, labels, temp_tib, info2, info_temp)
}
View(burger_week)
rm(i, j, description)
# cleaning
burger_week |>
mutate(description = case_when(is.na(description) ~ `NA`,
TRUE ~ description))
# cleaning
burger_week |>
mutate(description = case_when(is.na(description) ~ `NA`,
TRUE ~ description)) |> View()
# cleaning
burger_week |>
mutate(description = case_when(is.na(description) ~ `NA`,
TRUE ~ description),
address_hours = case_when(is.na(address_hours) ~ `NA`,
TRUE ~ address_hours)) |> View()
# cleaning
burger_week |>
mutate(description = case_when(is.na(description) ~ `NA`,
TRUE ~ description),
address_hours = case_when(is.na(address_hours) ~ `NA`,
TRUE ~ address_hours)) |>
select(-`NA`)
# export
save(burger_week, file = "burger_week.RData")
# cleaning
burger_week <- burger_week |>
mutate(description = case_when(is.na(description) ~ `NA`,
TRUE ~ description),
address_hours = case_when(is.na(address_hours) ~ `NA`,
TRUE ~ address_hours)) |>
select(-`NA`)
# export
save(burger_week, file = "burger_week.RData")
write_csv(x = burger_week, file = "burger_week_2024.csv")
# save to google sheets so we can vote:
gs4_create("burger-week-2024", sheets = burger_week)
url <- 'https://everout.com/portland/events/mercury-burger-week-2024/e180638/'
webpage <- read_html(url)
# get restaurant and burger names
restaurants <- html_nodes(webpage, '.text-center h4') |> html_text2()
burgers <- html_nodes(webpage, 'h3 a') |> html_text2()
img_links <- html_elements(webpage, '.img-fluid') %>% html_attr("src")
i = 1
url <- rest_links[i]
webpage <- read_html(url)
labels <- html_nodes(webpage, '.mb-5 > strong') |> html_text2() |> str_squish()
info <- html_nodes(webpage, '.mb-5 span') |> html_text2() |> str_squish()
info <- info[info != "The Fine Print!"]
if (length(labels) == 0) {
labels <- html_nodes(webpage, '.mb-5 strong') |> html_text2() |> str_squish()
}
labels <- labels[labels != "The Fine Print!"]
labels <- labels[labels != "AND"]
# build tibble
if (length(labels) != length(info)) {
description <- html_nodes(webpage, ".description") |> html_text2() |>
str_split("\\n")
description <- description[[1]]
description <- description[description != ""]
description <- description[description != "The Fine Print!"]
info2 <- c()
for (j in seq(1,length(description))) {
info_temp <- str_split_1(description[j], labels[j])
info2 = c(info2,info_temp[2] |> str_squish())
}
info <- info2
}
temp_tib = tibble(label = labels, info = info) |>
mutate(label2 = label_dict[label]) |>
select(-label) |> rename(label = label2) |>
pivot_wider(names_from = label, values_from = info) |>
mutate(image = img_links[i])
temp_tib
temp_tib$image |> pull()
temp_tib$image
source("~/GitHub Repos/food-weeks/burger week 2024/burger-week-2024.R", echo=TRUE)
View(burger_week_g)
